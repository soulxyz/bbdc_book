#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»markdownæ ¼å¼çš„å•è¯æœ¬ä¸­æå–å•è¯
"""

import re
import os
import sys
from bs4 import BeautifulSoup
import argparse
from pathlib import Path
import requests
import json
import time
from typing import List, Dict, Optional
from env_loader import load_env_variable, check_env_file_exists


class LLMWordCorrector:
    """LLMå•è¯æ›´æ­£ç±» - è°ƒç”¨ç¡…åŸºæµåŠ¨å¹³å°"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–LLMæ›´æ­£å™¨
        
        å‚æ•°:
            api_key: ç¡…åŸºæµåŠ¨APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡SILICONFLOW_API_KEYè¯»å–
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„ç¯å¢ƒå˜é‡åŠ è½½
        self.api_key = api_key or load_env_variable('SILICONFLOW_API_KEY')
        self.base_url = load_env_variable('SILICONFLOW_BASE_URL', "https://api.siliconflow.cn/v1/chat/completions")
        self.model = load_env_variable('SILICONFLOW_MODEL', "moonshotai/Kimi-K2-Instruct-0905")
        
        if not self.api_key:
            # æ£€æŸ¥ .env æ–‡ä»¶çŠ¶æ€
            exists, found_path, search_paths = check_env_file_exists()
            
            print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® SILICONFLOW_API_KEYï¼ŒLLMè‡ªåŠ¨æ›´æ­£åŠŸèƒ½å°†è¢«ç¦ç”¨")
            if exists:
                print(f"ğŸ’¡ æç¤º: æ‰¾åˆ° .env æ–‡ä»¶ ({found_path})ï¼Œä½†å…¶ä¸­æ²¡æœ‰ SILICONFLOW_API_KEY é…ç½®")
            else:
                print(f"ğŸ’¡ æç¤º: æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œè¯·åœ¨ exe æ‰€åœ¨ç›®å½•åˆ›å»º .env æ–‡ä»¶")
            print("   åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : SILICONFLOW_API_KEY=your_key_here")
            print("   è·å–åœ°å€: https://cloud.siliconflow.cn/")
    
    
    def is_enabled(self):
        """æ£€æŸ¥LLMåŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return bool(self.api_key)
    
    def correct_word(self, word, meaning="", context=""):
        """
        ä½¿ç”¨LLMæ›´æ­£å•è¯
        
        å‚æ•°:
            word: åŸå§‹å•è¯ï¼ˆå¯èƒ½æœ‰é”™è¯¯ï¼‰
            meaning: å•è¯çš„ä¸­æ–‡é‡Šä¹‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        è¿”å›:
            dict: {
                'success': bool,
                'original': str,
                'corrected': str,
                'confidence': str,
                'reason': str
            }
        """
        if not self.is_enabled():
            return {
                'success': False,
                'original': word,
                'corrected': word,
                'confidence': 'none',
                'reason': 'LLMåŠŸèƒ½æœªå¯ç”¨'
            }
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(word, meaning, context)
        
        try:
            response = requests.post(
                self.base_url,
                headers={
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model,
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­å•è¯æ‹¼å†™æ£€æŸ¥åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«å’Œä¿®æ­£è‹±è¯­å•è¯ä¸­çš„æ‹¼å†™é”™è¯¯ã€‚åªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    'temperature': 0.3,
                    'max_tokens': 200
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return self._parse_llm_response(word, content)
            else:
                return {
                    'success': False,
                    'original': word,
                    'corrected': word,
                    'confidence': 'none',
                    'reason': f'APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}'
                }
        
        except Exception as e:
            return {
                'success': False,
                'original': word,
                'corrected': word,
                'confidence': 'none',
                'reason': f'è°ƒç”¨LLMæ—¶å‡ºé”™: {str(e)}'
            }
    
    def _build_prompt(self, word, meaning, context):
        """æ„å»ºæç¤ºè¯"""
        prompt = f"""è¯·æ£€æŸ¥ä»¥ä¸‹è‹±è¯­å•è¯æ˜¯å¦æœ‰æ‹¼å†™é”™è¯¯ï¼Œå¦‚æœæœ‰é”™è¯¯è¯·ç»™å‡ºæ­£ç¡®çš„æ‹¼å†™ã€‚

åŸå§‹å•è¯: {word}
ä¸­æ–‡é‡Šä¹‰: {meaning}
{f'ä¸Šä¸‹æ–‡: {context}' if context else ''}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- corrected: æ›´æ­£åçš„å•è¯ï¼ˆå¦‚æœæ²¡æœ‰é”™è¯¯åˆ™è¿”å›åŸå•è¯ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼Œå¯é€‰å€¼ä¸º "high"ï¼ˆé«˜ï¼‰ã€"medium"ï¼ˆä¸­ï¼‰ã€"low"ï¼ˆä½ï¼‰
- reason: ç®€çŸ­è¯´æ˜æ›´æ­£çš„åŸå› æˆ–åˆ¤æ–­æ²¡æœ‰é”™è¯¯çš„ä¾æ®

ç¤ºä¾‹è¾“å‡ºï¼š
{{"corrected": "example", "confidence": "high", "reason": "åŸå•è¯æ‹¼å†™æ­£ç¡®"}}
æˆ–
{{"corrected": "receive", "confidence": "high", "reason": "ä¿®æ­£äº†iå’Œeçš„é¡ºåº"}}

åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        return prompt
    
    def _parse_llm_response(self, original_word, content):
        """è§£æLLMè¿”å›çš„ç»“æœ"""
        try:
            # å°è¯•æå–JSONå†…å®¹
            content = content.strip()
            
            # å¦‚æœåŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSON
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
            
            # è§£æJSON
            result = json.loads(content)
            
            corrected = result.get('corrected', original_word).strip()
            confidence = result.get('confidence', 'low')
            reason = result.get('reason', 'æ— è¯´æ˜')
            
            return {
                'success': True,
                'original': original_word,
                'corrected': corrected,
                'confidence': confidence,
                'reason': reason
            }
        
        except json.JSONDecodeError:
            # å¦‚æœæ— æ³•è§£æJSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å•è¯
            words = content.split()
            if words:
                return {
                    'success': True,
                    'original': original_word,
                    'corrected': words[0].strip('",.:;'),
                    'confidence': 'low',
                    'reason': 'ä»å“åº”ä¸­æå–çš„å•è¯'
                }
            else:
                return {
                    'success': False,
                    'original': original_word,
                    'corrected': original_word,
                    'confidence': 'none',
                    'reason': 'æ— æ³•è§£æLLMå“åº”'
                }
        except Exception as e:
            return {
                'success': False,
                'original': original_word,
                'corrected': original_word,
                'confidence': 'none',
                'reason': f'è§£æå“åº”æ—¶å‡ºé”™: {str(e)}'
            }
    
    def batch_correct(self, words_with_meanings, max_workers=3):
        """
        æ‰¹é‡æ›´æ­£å•è¯
        
        å‚æ•°:
            words_with_meanings: å•è¯åŠé‡Šä¹‰åˆ—è¡¨ [{'word': str, 'meaning': str}, ...]
            max_workers: æœ€å¤§å¹¶å‘æ•°
        
        è¿”å›:
            list: æ›´æ­£ç»“æœåˆ—è¡¨
        """
        results = []
        for item in words_with_meanings:
            word = item.get('word', '')
            meaning = item.get('meaning', '')
            result = self.correct_word(word, meaning)
            results.append(result)
            time.sleep(0.5)  # é¿å…APIé™æµ
        
        return results
    
    def generate_word_candidates(self, word, meaning):
        """
        ç”Ÿæˆå•è¯çš„å€™é€‰è¯ï¼ˆè¯æ ¹ã€æ´¾ç”Ÿè¯ç­‰ï¼‰
        
        å‚æ•°:
            word: åŸå§‹å•è¯
            meaning: å•è¯é‡Šä¹‰
        
        è¿”å›:
            dict: å€™é€‰è¯ä¿¡æ¯
        """
        if not self.is_enabled():
            return {
                'success': False,
                'candidates': [],
                'reason': 'LLMåŠŸèƒ½æœªå¯ç”¨'
            }
        
        prompt = f"""å¯¹äºæ— æ³•è¯†åˆ«çš„è‹±è¯­å•è¯"{word}"ï¼ˆé‡Šä¹‰ï¼š{meaning}ï¼‰ï¼Œè¯·ç”Ÿæˆ3-5ä¸ªå¯èƒ½çš„å€™é€‰è¯ã€‚

å€™é€‰è¯å¯ä»¥æ˜¯ï¼š
1. è¯¥å•è¯çš„è¯æ ¹æˆ–åŸºç¡€å½¢å¼
2. è¯¥å•è¯å»æ‰å‰ç¼€/åç¼€åçš„å½¢å¼
3. æ„æ€ç›¸è¿‘çš„å¸¸è§å•è¯
4. å¯èƒ½çš„æ­£ç¡®æ‹¼å†™ï¼ˆå¦‚æœåŸè¯æœ‰æ‹¼å†™é”™è¯¯ï¼‰

è¦æ±‚ï¼š
- å€™é€‰è¯å¿…é¡»æ˜¯çœŸå®å­˜åœ¨çš„å¸¸è§è‹±è¯­å•è¯
- ä¼˜å…ˆé€‰æ‹©æ›´åŸºç¡€ã€æ›´å¸¸ç”¨çš„è¯æ±‡
- ä¿æŒä¸åŸé‡Šä¹‰çš„ç›¸å…³æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ï¼š
- candidates: å€™é€‰è¯åˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å«wordå’Œreasonå­—æ®µï¼‰

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "candidates": [
    {{"word": "system", "reason": "supersystemçš„è¯æ ¹"}},
    {{"word": "efficient", "reason": "ineffectivelyçš„åä¹‰è¯æ ¹"}},
    {{"word": "finance", "reason": "finanziallyçš„è¯æ ¹"}}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = requests.post(
                self.base_url,
                headers={
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model,
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­è¯æ±‡åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„å•è¯ç”Ÿæˆåˆé€‚çš„å€™é€‰è¯ã€‚åªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    'temperature': 0.5,
                    'max_tokens': 300
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return self._parse_candidates_response(word, content)
            else:
                return {
                    'success': False,
                    'candidates': [],
                    'reason': f'APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}'
                }
        
        except Exception as e:
            return {
                'success': False,
                'candidates': [],
                'reason': f'è°ƒç”¨LLMæ—¶å‡ºé”™: {str(e)}'
            }
    
    def _parse_candidates_response(self, original_word, content):
        """è§£æå€™é€‰è¯å“åº”"""
        try:
            content = content.strip()
            
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
            candidates = result.get('candidates', [])
            
            return {
                'success': True,
                'original': original_word,
                'candidates': candidates,
                'reason': 'success'
            }
        
        except Exception as e:
            return {
                'success': False,
                'candidates': [],
                'reason': f'è§£æå“åº”å¤±è´¥: {str(e)}'
            }
    
    def select_best_candidate(self, original_word, meaning, candidates_with_status):
        """
        ä»å€™é€‰è¯ä¸­é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„ä¸€ä¸ª
        
        å‚æ•°:
            original_word: åŸå§‹å•è¯
            meaning: åŸå§‹é‡Šä¹‰
            candidates_with_status: å€™é€‰è¯åŠå…¶éªŒè¯çŠ¶æ€ [{'word': str, 'verified': bool, 'reason': str}, ...]
        
        è¿”å›:
            dict: æœ€ä½³å€™é€‰è¯ä¿¡æ¯
        """
        if not self.is_enabled():
            return {
                'success': False,
                'selected': None,
                'reason': 'LLMåŠŸèƒ½æœªå¯ç”¨'
            }
        
        # åªè€ƒè™‘éªŒè¯é€šè¿‡çš„å€™é€‰è¯
        verified_candidates = [c for c in candidates_with_status if c.get('verified', False)]
        
        if not verified_candidates:
            return {
                'success': False,
                'selected': None,
                'reason': 'æ²¡æœ‰éªŒè¯é€šè¿‡çš„å€™é€‰è¯'
            }
        
        if len(verified_candidates) == 1:
            return {
                'success': True,
                'selected': verified_candidates[0]['word'],
                'reason': 'åªæœ‰ä¸€ä¸ªéªŒè¯é€šè¿‡çš„å€™é€‰è¯',
                'confidence': 'high'
            }
        
        # æ„å»ºå€™é€‰è¯åˆ—è¡¨å­—ç¬¦ä¸²
        candidates_str = '\n'.join([
            f"{i+1}. {c['word']} - {c.get('reason', 'æ— è¯´æ˜')}"
            for i, c in enumerate(verified_candidates)
        ])
        
        prompt = f"""åŸå•è¯ï¼š{original_word}
é‡Šä¹‰ï¼š{meaning}

ä»¥ä¸‹å€™é€‰è¯éƒ½æ˜¯æœ‰æ•ˆçš„è‹±è¯­å•è¯ï¼š
{candidates_str}

è¯·ä»ä¸­é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§ã€æœ€å€¼å¾—å­¦ä¹ çš„ä¸€ä¸ªå•è¯ã€‚é€‰æ‹©æ ‡å‡†ï¼š
1. ä½¿ç”¨é¢‘ç‡æ›´é«˜çš„è¯
2. æ›´åŸºç¡€ã€æ›´æ ¸å¿ƒçš„è¯
3. èƒ½è¦†ç›–æ›´å¤šæ´¾ç”Ÿè¯çš„è¯æ ¹
4. ä¸åŸé‡Šä¹‰æœ€ç›¸å…³çš„è¯

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "selected": "é€‰ä¸­çš„å•è¯",
  "reason": "é€‰æ‹©ç†ç”±",
  "confidence": "high/medium/low"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = requests.post(
                self.base_url,
                headers={
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model,
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­æ•™å­¦ä¸“å®¶ã€‚å¸®åŠ©é€‰æ‹©æœ€å€¼å¾—å­¦ä¹ çš„å•è¯ã€‚åªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    'temperature': 0.3,
                    'max_tokens': 200
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return self._parse_selection_response(content)
            else:
                # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€™é€‰è¯
                return {
                    'success': True,
                    'selected': verified_candidates[0]['word'],
                    'reason': 'APIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€™é€‰è¯',
                    'confidence': 'low'
                }
        
        except Exception as e:
            return {
                'success': True,
                'selected': verified_candidates[0]['word'],
                'reason': f'é€‰æ‹©å¤±è´¥: {str(e)}ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€™é€‰è¯',
                'confidence': 'low'
            }
    
    def _parse_selection_response(self, content):
        """è§£æé€‰æ‹©å“åº”"""
        try:
            content = content.strip()
            
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
            
            return {
                'success': True,
                'selected': result.get('selected', ''),
                'reason': result.get('reason', 'æ— è¯´æ˜'),
                'confidence': result.get('confidence', 'medium')
            }
        
        except Exception as e:
            return {
                'success': False,
                'selected': None,
                'reason': f'è§£æå“åº”å¤±è´¥: {str(e)}'
            }


def batch_verify_candidates(bbdc_checker, candidates_list):
    """
    æ‰¹é‡éªŒè¯å€™é€‰è¯
    
    å‚æ•°:
        bbdc_checker: ä¸èƒŒå•è¯æ ¸å¯¹å™¨å®ä¾‹
        candidates_list: å€™é€‰è¯åˆ—è¡¨
    
    è¿”å›:
        dict: éªŒè¯ç»“æœï¼Œkeyä¸ºå•è¯ï¼Œvalueä¸ºæ˜¯å¦éªŒè¯é€šè¿‡
    """
    if not candidates_list:
        return {}
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_file = 'temp_candidates_verify.txt'
    try:
        with open(temp_file, 'w', encoding='utf-8') as f:
            for word in candidates_list:
                f.write(word + '\n')
        
        # éªŒè¯
        result = bbdc_checker.upload_word_file(temp_file)
        
        if "error" not in result:
            parsed = bbdc_checker.parse_result(result)
            if "error" not in parsed:
                recognized = set(w.lower() for w in parsed.get('recognized_words', []))
                return {word: (word.lower() in recognized) for word in candidates_list}
        
        return {word: False for word in candidates_list}
        
    finally:
        if os.path.exists(temp_file):
            os.remove(temp_file)


class BBDCWordChecker:
    """ä¸èƒŒå•è¯è¯ä¹¦æ ¸å¯¹ç±»"""
    
    def __init__(self):
        self.submit_url = "https://bbdc.cn/lexis/book/file/submit"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,zh-TW;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Origin': 'https://bbdc.cn',
            'Referer': 'https://bbdc.cn/lexis_book_index',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'X-Requested-With': 'XMLHttpRequest',
            'Connection': 'keep-alive'
        })
    
    def upload_word_file(self, file_path, filename=None):
        """ä¸Šä¼ å•è¯æ–‡ä»¶è¿›è¡Œæ ¸å¯¹"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not filename:
            filename = os.path.basename(file_path)
        
        try:
            with open(file_path, 'rb') as f:
                files = {
                    'file': (filename, f, 'text/plain')
                }
                
                response = self.session.post(
                    self.submit_url,
                    files=files,
                    timeout=30
                )
            
            if response.status_code == 200:
                try:
                    return response.json()
                except json.JSONDecodeError:
                    return {"error": "Invalid JSON response", "content": response.text}
            else:
                return {"error": f"HTTP {response.status_code}", "content": response.text}
                
        except requests.exceptions.Timeout:
            return {"error": "Request timeout"}
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def parse_result(self, result):
        """è§£ææ ¸å¯¹ç»“æœ"""
        if "error" in result:
            return result
        
        try:
            data_body = result.get("data_body", {})
            unknow_list = data_body.get("unknowList", "")
            know_list = data_body.get("knowList", "")
            
            # åˆ†å‰²å•è¯åˆ—è¡¨
            unrecognized_words = [word.strip() for word in unknow_list.split(',') if word.strip()]
            recognized_words = [word.strip() for word in know_list.split(',') if word.strip()]
            
            return {
                "unrecognized_words": unrecognized_words,
                "recognized_words": recognized_words,
                "unrecognized_count": len(unrecognized_words),
                "recognized_count": len(recognized_words),
                "total_count": len(unrecognized_words) + len(recognized_words)
            }
        except Exception as e:
            return {"error": f"è§£æç»“æœå¤±è´¥: {e}"}


def find_word_info_in_markdown(file_path, word):
    """
    åœ¨markdownæ–‡ä»¶ä¸­æŸ¥æ‰¾å•è¯çš„è¡Œå·å’Œå«ä¹‰
    
    å‚æ•°:
        file_path: markdownæ–‡ä»¶è·¯å¾„
        word: è¦æŸ¥æ‰¾çš„å•è¯
    
    è¿”å›:
        dict: åŒ…å«è¡Œå·å’Œå«ä¹‰çš„å­—å…¸
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        soup = BeautifulSoup(''.join(lines), 'html.parser')
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    word_text = cols[1].get_text(strip=True)
                    if word_text.lower() == word.lower():
                        meaning = cols[2].get_text(strip=True)
                        # æŸ¥æ‰¾åŸå§‹è¡Œå·
                        row_html = str(row)
                        for i, line in enumerate(lines):
                            if row_html in line:
                                return {
                                    'line_number': i + 1,
                                    'meaning': meaning,
                                    'word': word_text
                                }
        
        return None
    except Exception as e:
        print(f"æŸ¥æ‰¾å•è¯ {word} ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None


def apply_corrections_to_file(file_path, corrections):
    """
    å°†éªŒè¯é€šè¿‡çš„æ›´æ­£åº”ç”¨åˆ°æ–‡ä»¶
    
    å‚æ•°:
        file_path: å•è¯æ–‡ä»¶è·¯å¾„
        corrections: æ›´æ­£åˆ—è¡¨
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ›å»ºå¤‡ä»½
        backup_path = file_path + '.backup'
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {os.path.basename(backup_path)}")
        
        # åº”ç”¨æ›´æ­£
        lines = content.split('\n')
        correction_count = 0
        
        for correction in corrections:
            original = correction['original']
            corrected = correction['corrected']
            
            # åœ¨æ–‡ä»¶ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢
            for i, line in enumerate(lines):
                if line.strip() == original:
                    lines[i] = corrected
                    correction_count += 1
                    print(f"  âœ“ ç¬¬{i+1}è¡Œ: {original} â†’ {corrected}")
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        print(f"\nâœ… æˆåŠŸæ›¿æ¢ {correction_count} å¤„é”™è¯¯")
        return True
        
    except Exception as e:
        print(f"âŒ åº”ç”¨æ›´æ­£å¤±è´¥: {e}")
        return False


def auto_correct_with_llm(parsed_result, unrecognized_details, llm_corrector, bbdc_checker, original_file_path):
    """
    ä½¿ç”¨LLMè‡ªåŠ¨æ›´æ­£è¯†åˆ«å¤±è´¥çš„å•è¯ï¼Œå¹¶é‡æ–°éªŒè¯
    
    å‚æ•°:
        parsed_result: åŸå§‹æ ¸å¯¹ç»“æœ
        unrecognized_details: è¯†åˆ«å¤±è´¥çš„å•è¯è¯¦ç»†ä¿¡æ¯
        llm_corrector: LLMæ›´æ­£å™¨å®ä¾‹
        bbdc_checker: ä¸èƒŒå•è¯æ ¸å¯¹å™¨å®ä¾‹
        original_file_path: åŸå§‹å•è¯æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        dict: æ›´æ–°åçš„æ ¸å¯¹ç»“æœ
    """
    correction_results = []
    corrected_words = []  # æ›´æ­£åéœ€è¦éªŒè¯çš„å•è¯
    
    print("\n" + "=" * 60)
    print("ğŸ¤– LLMè‡ªåŠ¨æ›´æ­£å¤„ç†")
    print("=" * 60)
    
    # é€ä¸ªå¤„ç†è¯†åˆ«å¤±è´¥çš„å•è¯
    for i, detail in enumerate(unrecognized_details, 1):
        word = detail['word']
        meaning = detail.get('meaning', '')
        
        print(f"\n[{i}/{len(unrecognized_details)}] å¤„ç†å•è¯: {word}")
        
        # è°ƒç”¨LLMæ›´æ­£
        correction = llm_corrector.correct_word(word, meaning)
        correction['original_meaning'] = meaning
        correction['line_number'] = detail.get('line_number', 'æœªçŸ¥')
        correction_results.append(correction)
        
        if correction['success']:
            corrected_word = correction['corrected']
            print(f"  åŸå•è¯: {word}")
            print(f"  æ›´æ­£ä¸º: {corrected_word}")
            print(f"  ç½®ä¿¡åº¦: {correction['confidence']}")
            print(f"  åŸå› : {correction['reason']}")
            
            # å¦‚æœå•è¯è¢«æ›´æ­£äº†ï¼ˆä¸åŒäºåŸå•è¯ï¼‰
            if corrected_word.lower() != word.lower():
                corrected_words.append(corrected_word)
        else:
            print(f"  âŒ æ›´æ­£å¤±è´¥: {correction['reason']}")
    
    # å¦‚æœæœ‰æ›´æ­£åçš„å•è¯ï¼Œè¿›è¡ŒäºŒæ¬¡éªŒè¯
    verified_corrections = []
    if corrected_words:
        print(f"\n\nğŸ” æ­£åœ¨å¯¹ {len(corrected_words)} ä¸ªæ›´æ­£åçš„å•è¯è¿›è¡ŒéªŒè¯...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶åŒ…å«æ›´æ­£åçš„å•è¯
        temp_file = original_file_path.replace('.txt', '_llm_corrected_temp.txt')
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                for word in corrected_words:
                    f.write(word + '\n')
            
            # ä½¿ç”¨ä¸èƒŒå•è¯APIéªŒè¯æ›´æ­£åçš„å•è¯
            verify_result = bbdc_checker.upload_word_file(temp_file)
            
            if "error" not in verify_result:
                verify_parsed = bbdc_checker.parse_result(verify_result)
                
                if "error" not in verify_parsed:
                    verified_recognized = set(w.lower() for w in verify_parsed.get('recognized_words', []))
                    
                    # æ ‡è®°å“ªäº›æ›´æ­£åçš„å•è¯è¢«æˆåŠŸè¯†åˆ«
                    for correction in correction_results:
                        if correction['success']:
                            corrected_word = correction['corrected']
                            if corrected_word.lower() in verified_recognized:
                                correction['verified'] = True
                                correction['verification_status'] = 'âœ… éªŒè¯é€šè¿‡'
                                verified_corrections.append(correction)
                            else:
                                correction['verified'] = False
                                correction['verification_status'] = 'âŒ éªŒè¯å¤±è´¥'
                        else:
                            correction['verified'] = False
                            correction['verification_status'] = 'âš ï¸  æœªæ›´æ­£'
                else:
                    print(f"âš ï¸  éªŒè¯ç»“æœè§£æå¤±è´¥: {verify_parsed.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print(f"âš ï¸  éªŒè¯è¯·æ±‚å¤±è´¥: {verify_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
        except Exception as e:
            print(f"âš ï¸  éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
    
    # æ›´æ–°ç»“æœ
    parsed_result['llm_corrections'] = correction_results
    parsed_result['verified_corrections'] = verified_corrections
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_corrections = len(correction_results)
    successful_corrections = sum(1 for c in correction_results if c.get('verified', False))
    failed_corrections = [c for c in correction_results if not c.get('verified', False)]
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š LLMæ›´æ­£ç»Ÿè®¡:")
    print(f"  å°è¯•æ›´æ­£: {total_corrections} ä¸ªå•è¯")
    print(f"  éªŒè¯é€šè¿‡: {successful_corrections} ä¸ªå•è¯")
    print(f"  éªŒè¯å¤±è´¥: {len(failed_corrections)} ä¸ªå•è¯")
    if total_corrections > 0:
        print(f"  æˆåŠŸç‡: {successful_corrections/total_corrections*100:.1f}%")
    print("=" * 60)
    
    # å¯¹éªŒè¯å¤±è´¥çš„å•è¯è‡ªåŠ¨è¿›è¡ŒäºŒæ¬¡å¤„ç†ï¼ˆç”Ÿæˆå€™é€‰è¯ï¼‰
    second_round_results = []
    if failed_corrections and successful_corrections < total_corrections:
        print(f"\n\nğŸ”„ è‡ªåŠ¨å¯¹ {len(failed_corrections)} ä¸ªéªŒè¯å¤±è´¥çš„å•è¯è¿›è¡ŒäºŒæ¬¡å¤„ç†ï¼ˆç”Ÿæˆå€™é€‰è¯ï¼‰...")
        
        second_round_results = process_failed_corrections(
            failed_corrections, 
            llm_corrector, 
            bbdc_checker
        )
        
        if second_round_results:
            parsed_result['second_round_corrections'] = second_round_results
            # å°†äºŒæ¬¡å¤„ç†æˆåŠŸçš„ç»“æœä¹ŸåŠ å…¥verified_corrections
            for result in second_round_results:
                if result.get('selected_word'):
                    verified_corrections.append({
                        'original': result['original'],
                        'corrected': result['selected_word'],
                        'verified': True,
                        'verification_status': 'âœ… äºŒæ¬¡å¤„ç†éªŒè¯é€šè¿‡',
                        'confidence': result.get('confidence', 'medium'),
                        'reason': result.get('reason', ''),
                        'original_meaning': result.get('original_meaning', ''),
                        'line_number': result.get('line_number', 'æœªçŸ¥')
                    })
    
    # å¦‚æœæœ‰éªŒè¯é€šè¿‡çš„æ›´æ­£ï¼Œè‡ªåŠ¨åº”ç”¨
    all_corrections_to_apply = verified_corrections
    
    if all_corrections_to_apply:
        print(f"\n\nâœ… å…±æœ‰ {len(all_corrections_to_apply)} ä¸ªå•è¯å¯ä»¥æ›´æ­£ï¼")
        print("\næ›´æ­£åˆ—è¡¨ï¼š")
        for i, correction in enumerate(all_corrections_to_apply, 1):
            status_icon = "ğŸ”§" if "äºŒæ¬¡å¤„ç†" in correction.get('verification_status', '') else "âœ“"
            print(f"  {status_icon} {i}. {correction['original']} â†’ {correction['corrected']}")
            if "äºŒæ¬¡å¤„ç†" in correction.get('verification_status', ''):
                print(f"      è¯´æ˜: {correction.get('reason', '')}")
        
        print("\nğŸ”§ æ­£åœ¨è‡ªåŠ¨åº”ç”¨æ›´æ­£åˆ°æ–‡ä»¶...")
        success = apply_corrections_to_file(original_file_path, all_corrections_to_apply)
        if success:
            parsed_result['corrections_applied'] = True
            print("âœ… æ›´æ­£å·²æˆåŠŸåº”ç”¨åˆ°æ–‡ä»¶ï¼")
        else:
            parsed_result['corrections_applied'] = False
            print("âŒ åº”ç”¨æ›´æ­£å¤±è´¥")
    
    return parsed_result


def process_failed_corrections(failed_corrections, llm_corrector, bbdc_checker):
    """
    å¤„ç†éªŒè¯å¤±è´¥çš„æ›´æ­£ï¼Œç”Ÿæˆå€™é€‰è¯å¹¶é€‰æ‹©æœ€ä½³å€™é€‰
    
    å‚æ•°:
        failed_corrections: éªŒè¯å¤±è´¥çš„æ›´æ­£åˆ—è¡¨
        llm_corrector: LLMæ›´æ­£å™¨å®ä¾‹
        bbdc_checker: ä¸èƒŒå•è¯æ ¸å¯¹å™¨å®ä¾‹
    
    è¿”å›:
        list: äºŒæ¬¡å¤„ç†ç»“æœ
    """
    results = []
    
    print("\n" + "=" * 60)
    print("ğŸ”„ äºŒæ¬¡å¤„ç† - ç”Ÿæˆå€™é€‰è¯")
    print("=" * 60)
    
    for i, correction in enumerate(failed_corrections, 1):
        word = correction['original']
        meaning = correction.get('original_meaning', '')
        
        print(f"\n[{i}/{len(failed_corrections)}] å¤„ç†å•è¯: {word}")
        print(f"  é‡Šä¹‰: {meaning}")
        
        # ç”Ÿæˆå€™é€‰è¯
        candidates_result = llm_corrector.generate_word_candidates(word, meaning)
        
        if not candidates_result['success'] or not candidates_result['candidates']:
            print(f"  âš ï¸  ç”Ÿæˆå€™é€‰è¯å¤±è´¥: {candidates_result.get('reason', 'æœªçŸ¥åŸå› ')}")
            continue
        
        candidates = candidates_result['candidates']
        print(f"  ğŸ“ ç”Ÿæˆäº† {len(candidates)} ä¸ªå€™é€‰è¯:")
        for j, cand in enumerate(candidates, 1):
            print(f"     {j}. {cand['word']} - {cand.get('reason', '')}")
        
        # æå–å€™é€‰è¯åˆ—è¡¨
        candidate_words = [c['word'] for c in candidates]
        
        # æ‰¹é‡éªŒè¯å€™é€‰è¯
        print(f"  ğŸ” éªŒè¯å€™é€‰è¯...")
        verification_results = batch_verify_candidates(bbdc_checker, candidate_words)
        
        # æ ‡è®°éªŒè¯ç»“æœ
        candidates_with_status = []
        verified_count = 0
        for cand in candidates:
            word_text = cand['word']
            is_verified = verification_results.get(word_text, False)
            candidates_with_status.append({
                'word': word_text,
                'verified': is_verified,
                'reason': cand.get('reason', '')
            })
            if is_verified:
                verified_count += 1
                print(f"     âœ“ {word_text} - éªŒè¯é€šè¿‡")
            else:
                print(f"     âœ— {word_text} - éªŒè¯å¤±è´¥")
        
        if verified_count == 0:
            print(f"  âŒ æ‰€æœ‰å€™é€‰è¯éƒ½éªŒè¯å¤±è´¥")
            continue
        
        # é€‰æ‹©æœ€ä½³å€™é€‰è¯
        print(f"  ğŸ¤– AIé€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„å•è¯...")
        selection_result = llm_corrector.select_best_candidate(word, meaning, candidates_with_status)
        
        if selection_result['success'] and selection_result['selected']:
            selected = selection_result['selected']
            print(f"  âœ… é€‰æ‹©: {selected}")
            print(f"     ç†ç”±: {selection_result.get('reason', '')}")
            print(f"     ç½®ä¿¡åº¦: {selection_result.get('confidence', '')}")
            
            results.append({
                'original': word,
                'selected_word': selected,
                'candidates': candidates_with_status,
                'reason': selection_result.get('reason', ''),
                'confidence': selection_result.get('confidence', ''),
                'original_meaning': meaning,
                'line_number': correction.get('line_number', 'æœªçŸ¥')
            })
        else:
            print(f"  âš ï¸  é€‰æ‹©å¤±è´¥: {selection_result.get('reason', '')}")
        
        time.sleep(0.5)  # é¿å…APIé™æµ
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š äºŒæ¬¡å¤„ç†ç»Ÿè®¡:")
    print(f"  å¤„ç†å•è¯: {len(failed_corrections)} ä¸ª")
    print(f"  æˆåŠŸé€‰æ‹©: {len(results)} ä¸ª")
    if len(failed_corrections) > 0:
        print(f"  æˆåŠŸç‡: {len(results)/len(failed_corrections)*100:.1f}%")
    print("=" * 60)
    
    return results


def check_words_with_bbdc(file_path, words_list, original_md_file, use_llm=True):
    """
    ä½¿ç”¨ä¸èƒŒå•è¯æ ¸å¯¹å•è¯åˆ—è¡¨
    
    å‚æ•°:
        file_path: ä¸´æ—¶å•è¯æ–‡ä»¶è·¯å¾„
        words_list: å•è¯åˆ—è¡¨
        original_md_file: åŸå§‹markdownæ–‡ä»¶è·¯å¾„
        use_llm: æ˜¯å¦ä½¿ç”¨LLMè‡ªåŠ¨æ›´æ­£è¯†åˆ«å¤±è´¥çš„å•è¯
    
    è¿”å›:
        dict: æ ¸å¯¹ç»“æœ
    """
    print("\nğŸ”„ æ­£åœ¨ä½¿ç”¨ä¸èƒŒå•è¯æ ¸å¯¹å•è¯...")
    
    checker = BBDCWordChecker()
    result = checker.upload_word_file(file_path)
    
    if "error" in result:
        print(f"âŒ æ ¸å¯¹å¤±è´¥: {result['error']}")
        return None
    
    parsed_result = checker.parse_result(result)
    
    if "error" in parsed_result:
        print(f"âŒ è§£æç»“æœå¤±è´¥: {parsed_result['error']}")
        return None
    
    # ä¸ºè¯†åˆ«ä¸æˆåŠŸçš„å•è¯æŸ¥æ‰¾è¯¦ç»†ä¿¡æ¯
    unrecognized_details = []
    for word in parsed_result['unrecognized_words']:
        word_info = find_word_info_in_markdown(original_md_file, word)
        if word_info:
            unrecognized_details.append(word_info)
        else:
            unrecognized_details.append({
                'line_number': 'æœªçŸ¥',
                'meaning': 'æœªæ‰¾åˆ°',
                'word': word
            })
    
    parsed_result['unrecognized_details'] = unrecognized_details
    parsed_result['original_file'] = original_md_file
    
    # ä½¿ç”¨LLMè‡ªåŠ¨æ›´æ­£è¯†åˆ«å¤±è´¥çš„å•è¯
    if use_llm and parsed_result['unrecognized_details']:
        print(f"\nğŸ¤– æ£€æµ‹åˆ° {len(parsed_result['unrecognized_details'])} ä¸ªè¯†åˆ«å¤±è´¥çš„å•è¯ï¼Œæ­£åœ¨ä½¿ç”¨LLMå°è¯•è‡ªåŠ¨æ›´æ­£...")
        llm_corrector = LLMWordCorrector()
        
        if llm_corrector.is_enabled():
            parsed_result = auto_correct_with_llm(
                parsed_result, 
                unrecognized_details, 
                llm_corrector, 
                checker, 
                file_path
            )
        else:
            print("âš ï¸  LLMåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨æ›´æ­£")
    
    return parsed_result


def extract_words_from_markdown(file_path, output_file=None, include_phrases=False):
    """
    ä»markdownæ–‡ä»¶ä¸­æå–å•è¯
    
    å‚æ•°:
        file_path: markdownæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        include_phrases: æ˜¯å¦åŒ…å«çŸ­è¯­ï¼ˆé»˜è®¤Falseï¼‰
    
    è¿”å›:
        words_list: æå–çš„å•è¯åˆ—è¡¨
    """
    # è¯»å–æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨BeautifulSoupè§£æHTMLè¡¨æ ¼
    soup = BeautifulSoup(content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
    tables = soup.find_all('table')
    
    words_data = []  # å­˜å‚¨å•è¯åŠå…¶ä¿¡æ¯
    phrases_data = []  # å­˜å‚¨çŸ­è¯­åŠå…¶ä¿¡æ¯
    
    for table in tables:
        rows = table.find_all('tr')
        
        # è·³è¿‡è¡¨å¤´å’Œç©ºè¡Œ
        is_phrase_table = False
        for row in rows:
            cols = row.find_all('td')
            
            # è‡³å°‘éœ€è¦3åˆ—ï¼šåºå·ã€å•è¯/çŸ­è¯­ã€è¯ä¹‰
            if len(cols) >= 3:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨å¤´
                col1_text = cols[0].get_text(strip=True)
                col2_text = cols[1].get_text(strip=True)
                
                # è·³è¿‡è¡¨å¤´è¡Œå’Œè¡¥å……åŒº
                if col1_text in ['NO.', ''] or 'è¡¥å……åŒº' in col1_text:
                    continue
                
                # åˆ¤æ–­æ˜¯çŸ­è¯­è¡¨è¿˜æ˜¯å•è¯è¡¨
                if col2_text in ['å•è¯', 'çŸ­è¯­']:
                    if col2_text == 'çŸ­è¯­':
                        is_phrase_table = True
                    continue
                
                # æå–æ•°æ®
                try:
                    number = col1_text
                    word_or_phrase = col2_text
                    meaning = cols[2].get_text(strip=True)
                    
                    # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆæ•°æ®
                    if not word_or_phrase or not number.isdigit():
                        continue
                    
                    # åˆ¤æ–­æ˜¯å•è¯è¿˜æ˜¯çŸ­è¯­ï¼ˆé€šè¿‡ç©ºæ ¼åˆ¤æ–­ï¼‰
                    if ' ' in word_or_phrase or '-' in word_or_phrase:
                        phrases_data.append({
                            'number': number,
                            'phrase': word_or_phrase,
                            'meaning': meaning
                        })
                    else:
                        words_data.append({
                            'number': number,
                            'word': word_or_phrase,
                            'meaning': meaning
                        })
                except:
                    continue
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"æå–åˆ° {len(words_data)} ä¸ªå•è¯")
    print(f"æå–åˆ° {len(phrases_data)} ä¸ªçŸ­è¯­")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥å•è¯
            f.write("=" * 50 + "\n")
            f.write("å•è¯åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
            for item in words_data:
                f.write(f"{item['number']}. {item['word']}\t{item['meaning']}\n")
            
            if include_phrases:
                f.write("\n" + "=" * 50 + "\n")
                f.write("çŸ­è¯­åˆ—è¡¨\n")
                f.write("=" * 50 + "\n\n")
                for item in phrases_data:
                    f.write(f"{item['number']}. {item['phrase']}\t{item['meaning']}\n")
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # è¿”å›ç»“æœ
    if include_phrases:
        return words_data, phrases_data
    else:
        return words_data


def extract_words_only(file_path, output_file=None, unique=True, auto_check=True):
    """
    ä»…æå–å•è¯ï¼ˆä¸åŒ…å«è¯ä¹‰ï¼‰ï¼Œæ¯è¡Œä¸€ä¸ªå•è¯
    
    å‚æ•°:
        file_path: markdownæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        unique: æ˜¯å¦å»é‡ï¼ˆé»˜è®¤Trueï¼‰
        auto_check: æ˜¯å¦è‡ªåŠ¨è¿›è¡Œä¸èƒŒå•è¯æ ¸å¯¹ï¼ˆé»˜è®¤Trueï¼‰
    
    è¿”å›:
        words_list: å•è¯åˆ—è¡¨
    """
    words_data = extract_words_from_markdown(file_path)
    
    # æå–æ‰€æœ‰å•è¯
    words_list = [item['word'] for item in words_data]
    
    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    if unique:
        seen = set()
        unique_words = []
        for word in words_list:
            if word.lower() not in seen:
                seen.add(word.lower())
                unique_words.append(word)
        words_list = unique_words
        print(f"å»é‡åå‰©ä½™ {len(words_list)} ä¸ªå•è¯")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            for word in words_list:
                f.write(word + '\n')
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # è‡ªåŠ¨è¿›è¡Œä¸èƒŒå•è¯æ ¸å¯¹
        if auto_check and len(words_list) > 0:
            check_result = check_words_with_bbdc(output_file, words_list, file_path)
            if check_result:
                print_check_result(check_result, output_file)
    
    return words_list


def print_check_result(check_result, output_file):
    """
    æ‰“å°ä¸èƒŒå•è¯æ ¸å¯¹ç»“æœ
    
    å‚æ•°:
        check_result: æ ¸å¯¹ç»“æœ
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸èƒŒå•è¯è¯ä¹¦æ ¸å¯¹ç»“æœ")
    print("=" * 60)
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»å•è¯æ•°: {check_result['total_count']}")
    print(f"  è¯†åˆ«æˆåŠŸ: {check_result['recognized_count']}")
    print(f"  è¯†åˆ«ä¸æˆåŠŸ: {check_result['unrecognized_count']}")
    print(f"  è¯†åˆ«æˆåŠŸç‡: {check_result['recognized_count']/check_result['total_count']*100:.1f}%")
    
    # æ˜¾ç¤ºLLMæ›´æ­£ç»Ÿè®¡
    if 'llm_corrections' in check_result:
        corrections = check_result['llm_corrections']
        verified = check_result.get('verified_corrections', [])
        second_round = check_result.get('second_round_corrections', [])
        
        print(f"\nğŸ¤– LLMè‡ªåŠ¨æ›´æ­£:")
        print(f"  å°è¯•æ›´æ­£: {len(corrections)} ä¸ªå•è¯")
        first_round_verified = len([v for v in verified if 'äºŒæ¬¡å¤„ç†' not in v.get('verification_status', '')])
        print(f"  éªŒè¯é€šè¿‡: {first_round_verified} ä¸ªå•è¯")
        
        if second_round:
            print(f"\nğŸ”„ äºŒæ¬¡å¤„ç†ï¼ˆå€™é€‰è¯ï¼‰:")
            print(f"  å¤„ç†å•è¯: {len([c for c in corrections if not c.get('verified', False)])} ä¸ª")
            print(f"  æˆåŠŸé€‰æ‹©: {len(second_round)} ä¸ªå•è¯")
        
        if check_result.get('corrections_applied'):
            print(f"\n  âœ… å·²åº”ç”¨æ›´æ­£åˆ°æ–‡ä»¶")
    
    # æ˜¾ç¤ºè¯†åˆ«æˆåŠŸçš„å•è¯ï¼ˆå‰10ä¸ªï¼‰
    if check_result['recognized_words']:
        print(f"\nâœ… è¯†åˆ«æˆåŠŸçš„å•è¯ï¼ˆå‰10ä¸ªï¼‰:")
        for i, word in enumerate(check_result['recognized_words'][:10], 1):
            print(f"  {i:2d}. {word}")
        
        if len(check_result['recognized_words']) > 10:
            print(f"  ... è¿˜æœ‰ {len(check_result['recognized_words']) - 10} ä¸ªè¯†åˆ«æˆåŠŸçš„å•è¯")
    
    # æ˜¾ç¤ºLLMæ›´æ­£è¯¦æƒ…
    if 'llm_corrections' in check_result and check_result['llm_corrections']:
        print(f"\n\nğŸ¤– LLMæ›´æ­£è¯¦æƒ…:")
        print("=" * 80)
        for i, correction in enumerate(check_result['llm_corrections'], 1):
            status = correction.get('verification_status', 'æœªéªŒè¯')
            print(f"\n  [{i}] {correction['original']} â†’ {correction['corrected']}")
            print(f"      çŠ¶æ€: {status}")
            print(f"      ç½®ä¿¡åº¦: {correction.get('confidence', 'unknown')}")
            print(f"      åŸå› : {correction.get('reason', 'æ— ')}")
            print(f"      é‡Šä¹‰: {correction.get('original_meaning', 'æ— ')}")
            print(f"      è¡Œå·: ç¬¬{correction.get('line_number', 'æœªçŸ¥')}è¡Œ")
        print("=" * 80)
    
    # æ˜¾ç¤ºäºŒæ¬¡å¤„ç†è¯¦æƒ…
    if 'second_round_corrections' in check_result and check_result['second_round_corrections']:
        print(f"\n\nğŸ”„ äºŒæ¬¡å¤„ç†ï¼ˆå€™é€‰è¯é€‰æ‹©ï¼‰è¯¦æƒ…:")
        print("=" * 80)
        for i, result in enumerate(check_result['second_round_corrections'], 1):
            print(f"\n  [{i}] {result['original']} â†’ {result['selected_word']}")
            print(f"      å€™é€‰è¯:")
            for cand in result.get('candidates', []):
                status = "âœ“" if cand.get('verified') else "âœ—"
                print(f"        {status} {cand['word']} - {cand.get('reason', '')}")
            print(f"      AIé€‰æ‹©: {result['selected_word']}")
            print(f"      ç†ç”±: {result.get('reason', 'æ— ')}")
            print(f"      ç½®ä¿¡åº¦: {result.get('confidence', 'unknown')}")
            print(f"      é‡Šä¹‰: {result.get('original_meaning', 'æ— ')}")
            print(f"      è¡Œå·: ç¬¬{result.get('line_number', 'æœªçŸ¥')}è¡Œ")
        print("=" * 80)
    
    # æ˜¾ç¤ºè¯†åˆ«ä¸æˆåŠŸçš„å•è¯ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰
    if check_result['unrecognized_details']:
        source_file = os.path.basename(check_result.get('original_file', 'æœªçŸ¥æ–‡ä»¶'))
        word_file = os.path.basename(output_file)
        print(f"\n\næ£€æŸ¥è¿™äº›å•è¯æ˜¯å¦æœ‰è¯¯,ä½ å¯ä»¥é€šè¿‡æ ‡æ˜çš„è¡Œæ•°ç›´æ¥é˜…è¯»è¯¥è¡Œå‰åä»¥ç¡®å®šå…¶ä½ç½®ï¼Œä¸ç”¨é˜…è¯»æ•´ä¸ªæ–‡ä»¶,å¦‚æœå‘ç°é”™åˆ«å­—ç­‰æƒ…å†µã€è¯·è¯¢é—®æˆ‘å¾—åˆ°åŒæ„åï¼Œå¯¹å•è¯æ–‡æœ¬æ–‡ä»¶è¿›è¡Œä¿®æ­£ã€‚\n è¯†åˆ«ä¸æˆåŠŸçš„å•è¯ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰,æºæ–‡ä»¶ã€{source_file}ã€‘ï¼Œå•è¯æ–‡æœ¬æ–‡ä»¶ã€{word_file}ã€‘:")
        for i, detail in enumerate(check_result['unrecognized_details'], 1):
            print(f"  {i:2d}. {detail['word']:<15} ç¬¬{detail['line_number']}è¡Œ  {detail['meaning']}")
    
    # è‡ªåŠ¨ä¿å­˜è¯¦ç»†ç»“æœ
    if check_result.get('llm_corrections') or check_result['unrecognized_count'] > 0:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        detail_output = f"bbdc_check_detail_{timestamp}.txt"
        
        print(f"\nğŸ’¾ è‡ªåŠ¨ä¿å­˜è¯¦ç»†ç»“æœåˆ°: {detail_output}")
        
        try:
            with open(detail_output, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("ä¸èƒŒå•è¯è¯ä¹¦æ ¸å¯¹è¯¦ç»†ç»“æœ\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  æ€»å•è¯æ•°: {check_result['total_count']}\n")
                f.write(f"  è¯†åˆ«æˆåŠŸ: {check_result['recognized_count']}\n")
                f.write(f"  è¯†åˆ«ä¸æˆåŠŸ: {check_result['unrecognized_count']}\n")
                f.write(f"  è¯†åˆ«æˆåŠŸç‡: {check_result['recognized_count']/check_result['total_count']*100:.1f}%\n\n")
                
                # ä¿å­˜LLMæ›´æ­£ä¿¡æ¯
                if 'llm_corrections' in check_result and check_result['llm_corrections']:
                    f.write("\n" + "=" * 30 + " LLMè‡ªåŠ¨æ›´æ­£è¯¦æƒ… " + "=" * 30 + "\n\n")
                    if check_result.get('corrections_applied'):
                        f.write("âœ… æ›´æ­£å·²åº”ç”¨åˆ°æ–‡ä»¶\n\n")
                    for i, correction in enumerate(check_result['llm_corrections'], 1):
                        f.write(f"{i:3d}. {correction['original']} â†’ {correction['corrected']}\n")
                        f.write(f"     çŠ¶æ€: {correction.get('verification_status', 'æœªéªŒè¯')}\n")
                        f.write(f"     ç½®ä¿¡åº¦: {correction.get('confidence', 'unknown')}\n")
                        f.write(f"     åŸå› : {correction.get('reason', 'æ— ')}\n")
                        f.write(f"     é‡Šä¹‰: {correction.get('original_meaning', 'æ— ')}\n")
                        f.write(f"     è¡Œå·: ç¬¬{correction.get('line_number', 'æœªçŸ¥')}è¡Œ\n\n")
                
                f.write("=" * 30 + " è¯†åˆ«æˆåŠŸçš„å•è¯ " + "=" * 30 + "\n")
                for i, word in enumerate(check_result['recognized_words'], 1):
                    f.write(f"{i:3d}. {word}\n")
                
                f.write("\n" + "=" * 30 + " è¯†åˆ«ä¸æˆåŠŸçš„å•è¯ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰ " + "=" * 30 + "\n")
                f.write(f"æºæ–‡ä»¶: {os.path.basename(check_result.get('original_file', 'æœªçŸ¥æ–‡ä»¶'))}\n")
                f.write(f"å•è¯æ–‡æœ¬æ–‡ä»¶: {os.path.basename(output_file)}\n\n")
                for i, detail in enumerate(check_result['unrecognized_details'], 1):
                    f.write(f"{i:3d}. {detail['word']:<20} ç¬¬{detail['line_number']}è¡Œ  {detail['meaning']}\n")
            
            print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è¯¦ç»†ç»“æœå¤±è´¥: {e}")
    else:
        print("\nğŸ’¡ æç¤º: æ‰€æœ‰å•è¯éƒ½å·²è¯†åˆ«æˆåŠŸï¼Œæ— éœ€ä¿å­˜è¯¦ç»†æŠ¥å‘Š")


def print_header():
    """æ‰“å°ç¨‹åºå¤´éƒ¨"""
    print("\n" + "=" * 60)
    print("           ğŸ“š å•è¯æå–å·¥å…· - Word Extractor")
    print("           æ”¯æŒ PDF å’Œ Markdown æ–‡ä»¶")
    print("=" * 60)
    print()


def print_menu():
    """æ‰“å°ä¸»èœå•"""
    print("\nè¯·é€‰æ‹©æ“ä½œæ¨¡å¼ï¼š")
    print("  [1] æå–å•è¯ï¼ˆä»…å•è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼‰+ è‡ªåŠ¨æ ¸å¯¹")
    print("  [2] æå–å•è¯ï¼ˆåŒ…å«è¯ä¹‰ï¼‰")
    print("  [3] æå–å•è¯å’ŒçŸ­è¯­ï¼ˆåŒ…å«è¯ä¹‰ï¼‰")
    print("  [4] æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶")
    print("  [0] é€€å‡º")
    print()


def get_choice(prompt, valid_choices):
    """è·å–ç”¨æˆ·é€‰æ‹©"""
    while True:
        choice = input(prompt).strip()
        if choice in valid_choices:
            return choice
        print(f"âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥ {', '.join(valid_choices)} ä¸­çš„ä¸€ä¸ª")


def get_input_file():
    """è·å–è¾“å…¥æ–‡ä»¶è·¯å¾„"""
    print("\nğŸ“‚ è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼š")
    print("  æç¤ºï¼šå¯ä»¥ç›´æ¥æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤çª—å£ï¼Œæˆ–è¾“å…¥å®Œæ•´è·¯å¾„")
    print("  æ”¯æŒæ ¼å¼ï¼šPDFã€Markdown (.md)")
    
    while True:
        file_path = input("æ–‡ä»¶è·¯å¾„: ").strip().strip('"').strip("'")
        
        if not file_path:
            print("âŒ è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            retry = input("æ˜¯å¦é‡æ–°è¾“å…¥ï¼Ÿ(y/n): ").strip().lower()
            if retry != 'y':
                return None
            continue
        
        # æ”¯æŒ PDF å’Œ MD æ ¼å¼
        if not (file_path.lower().endswith('.md') or file_path.lower().endswith('.pdf')):
            print("âš ï¸  è­¦å‘Šï¼šæ–‡ä»¶ä¸æ˜¯ .md æˆ– .pdf æ ¼å¼ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ", end='')
            if input().strip().lower() != 'y':
                continue
        
        return file_path


def get_output_file(default_name):
    """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
    print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶åï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤: {default_name}ï¼‰: ", end='')
    output = input().strip().strip('"').strip("'")
    
    if not output:
        return default_name
    
    # ç¡®ä¿æœ‰æ–‡ä»¶æ‰©å±•å
    if not output.endswith('.txt'):
        output += '.txt'
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è¦†ç›–
    if os.path.exists(output):
        print(f"âš ï¸  æ–‡ä»¶ {output} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ", end='')
        if input().strip().lower() != 'y':
            return get_output_file(default_name)
    
    return output


def preview_results(words_data, phrases_data=None, limit=10):
    """é¢„è§ˆæå–ç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ æå–ç»“æœé¢„è§ˆï¼ˆå‰ {} é¡¹ï¼‰".format(limit))
    print("=" * 60)
    
    if words_data:
        print("\nã€å•è¯ã€‘")
        for i, item in enumerate(words_data[:limit], 1):
            if isinstance(item, dict):
                word = item.get('word', '')
                meaning = item.get('meaning', '')
                print(f"  {i}. {word:<20} {meaning}")
            else:
                print(f"  {i}. {item}")
        
        if len(words_data) > limit:
            print(f"  ... è¿˜æœ‰ {len(words_data) - limit} ä¸ªå•è¯æœªæ˜¾ç¤º")
    
    if phrases_data:
        print("\nã€çŸ­è¯­ã€‘")
        for i, item in enumerate(phrases_data[:limit], 1):
            phrase = item.get('phrase', '')
            meaning = item.get('meaning', '')
            print(f"  {i}. {phrase:<30} {meaning}")
        
        if len(phrases_data) > limit:
            print(f"  ... è¿˜æœ‰ {len(phrases_data) - limit} ä¸ªçŸ­è¯­æœªæ˜¾ç¤º")
    
    print()


def process_pdf_file(pdf_path, output_dir=None):
    """
    å¤„ç† PDF æ–‡ä»¶ï¼šé€šè¿‡ Mineru API è½¬æ¢ä¸º Markdown å¹¶æå–å•è¯
    
    å‚æ•°:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›:
        str: ç”Ÿæˆçš„ markdown æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    try:
        from mineru_api import MineruWordExtractor
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥ mineru_api æ¨¡å—ï¼Œè¯·ç¡®ä¿ mineru_api.py æ–‡ä»¶å­˜åœ¨")
        return None
    
    print(f"\n{'='*60}")
    print(f"ğŸ“„ æ£€æµ‹åˆ° PDF æ–‡ä»¶ï¼Œå°†é€šè¿‡ Mineru API å¤„ç†")
    print(f"{'='*60}\n")
    
    try:
        # åˆ›å»º Mineru æå–å™¨
        extractor = MineruWordExtractor()
        
        # å¤„ç† PDF
        result = extractor.process_local_pdf(
            pdf_path,
            output_dir=output_dir,
            auto_extract_words=True,
            is_ocr=True
        )
        
        if result.get('success'):
            # è·å–ç”Ÿæˆçš„ markdown æ–‡ä»¶
            markdown_files = result.get('markdown_files', [])
            if markdown_files:
                return markdown_files[0]  # è¿”å›ç¬¬ä¸€ä¸ª markdown æ–‡ä»¶
        else:
            print(f"âŒ PDF å¤„ç†å¤±è´¥: {result.get('error')}")
            return None
    
    except ValueError as e:
        print(f"âŒ {str(e)}")
        print("\nğŸ’¡ è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® MINERU_API_TOKEN")
        print("   Token è·å–åœ°å€: https://mineru.net/")
        return None
    except Exception as e:
        print(f"âŒ å¤„ç† PDF æ—¶å‡ºé”™: {e}")
        return None


def find_markdown_files(directory='.'):
    """æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶"""
    md_files = list(Path(directory).glob('*.md'))
    return [str(f) for f in md_files]


def batch_process():
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    print("\nğŸ“ æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("æ­£åœ¨æœç´¢å½“å‰ç›®å½•ä¸‹çš„ .md æ–‡ä»¶...")
    
    md_files = find_markdown_files()
    
    if not md_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• .md æ–‡ä»¶")
        return
    
    print(f"\næ‰¾åˆ° {len(md_files)} ä¸ªæ–‡ä»¶ï¼š")
    for i, file in enumerate(md_files, 1):
        print(f"  [{i}] {file}")
    
    print("\næ˜¯å¦å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Ÿ(y/n): ", end='')
    if input().strip().lower() != 'y':
        return
    
    mode = get_choice("\né€‰æ‹©æå–æ¨¡å¼ï¼š\n  [1] ä»…å•è¯\n  [2] å•è¯+è¯ä¹‰\n  [3] å•è¯+çŸ­è¯­+è¯ä¹‰\næ¨¡å¼: ", ['1', '2', '3'])
    
    success_count = 0
    for file in md_files:
        try:
            print(f"\nå¤„ç†: {file}")
            base_name = Path(file).stem
            
            if mode == '1':
                output = f"{base_name}_å•è¯.txt"
                extract_words_only(file, output, unique=True)
            elif mode == '2':
                output = f"{base_name}_å•è¯è¯ä¹‰.txt"
                extract_words_from_markdown(file, output, include_phrases=False)
            else:
                output = f"{base_name}_å®Œæ•´.txt"
                extract_words_from_markdown(file, output, include_phrases=True)
            
            success_count += 1
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
    
    print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(md_files)} ä¸ªæ–‡ä»¶")


def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print_header()
    
    while True:
        print_menu()
        print("é»˜è®¤é€‰æ‹©é€‰é¡¹1ï¼ˆæå–å•è¯+è‡ªåŠ¨æ ¸å¯¹ï¼‰")
        choice = input("è¯·é€‰æ‹© [0-4]ï¼ˆç›´æ¥å›è½¦é€‰æ‹©1ï¼‰: ").strip()
        
        # å¦‚æœç”¨æˆ·ç›´æ¥å›è½¦ï¼Œé»˜è®¤é€‰æ‹©1
        if not choice:
            choice = '1'
            print("å·²é€‰æ‹©é€‰é¡¹1")
        
        if choice not in ['0', '1', '2', '3', '4']:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥ 0, 1, 2, 3, 4 ä¸­çš„ä¸€ä¸ª")
            continue
        
        if choice == '0':
            print("\nğŸ‘‹ å†è§ï¼")
            break
        
        if choice == '4':
            batch_process()
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            continue
        
        # è·å–è¾“å…¥æ–‡ä»¶
        input_file = get_input_file()
        if not input_file:
            continue
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        is_pdf = input_file.lower().endswith('.pdf')
        
        # å¦‚æœæ˜¯ PDFï¼Œå…ˆè½¬æ¢ä¸º Markdown
        if is_pdf:
            print("\nğŸ”„ æ­£åœ¨é€šè¿‡ Mineru API å¤„ç† PDF...")
            markdown_file = process_pdf_file(input_file)
            
            if not markdown_file:
                print("âŒ PDF å¤„ç†å¤±è´¥")
                input("\næŒ‰å›è½¦é”®ç»§ç»­...")
                continue
            
            print(f"âœ… PDF å·²è½¬æ¢ä¸º Markdown: {os.path.basename(markdown_file)}")
            print(f"â„¹ï¸  å•è¯å·²è‡ªåŠ¨æå–å’Œæ ¸å¯¹ï¼ŒæŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶")
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            continue
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
        base_name = Path(input_file).stem
        
        if choice == '1':
            # ä»…å•è¯æ¨¡å¼ + è‡ªåŠ¨æ ¸å¯¹
            default_output = f"{base_name}_å•è¯.txt"
            output_file = get_output_file(default_output)
            
            print("\næ˜¯å¦å»é‡ï¼Ÿ(y/nï¼Œé»˜è®¤y): ", end='')
            unique = input().strip().lower() != 'n'
            
            print("\nğŸ”„ æ­£åœ¨æå–å•è¯...")
            try:
                words = extract_words_only(input_file, output_file, unique, auto_check=True)
                print(f"\nâœ… æˆåŠŸï¼å…±æå– {len(words)} ä¸ªå•è¯")
                
                # é¢„è§ˆï¼ˆå¦‚æœå·²ç»è¿›è¡Œäº†æ ¸å¯¹ï¼Œè¿™é‡Œå°±ä¸éœ€è¦å†é¢„è§ˆäº†ï¼‰
                if not os.path.exists(output_file.replace('.txt', '_check_detail_').replace('.txt', '') + '*.txt'):
                    print("\nå‰10ä¸ªå•è¯ï¼š")
                    for i, word in enumerate(words[:10], 1):
                        print(f"  {i}. {word}")
                    if len(words) > 10:
                        print(f"  ... è¿˜æœ‰ {len(words) - 10} ä¸ª")
                
            except Exception as e:
                print(f"\nâŒ æå–å¤±è´¥: {e}")
        
        elif choice == '2':
            # å•è¯+è¯ä¹‰æ¨¡å¼
            default_output = f"{base_name}_å•è¯è¯ä¹‰.txt"
            output_file = get_output_file(default_output)
            
            print("\nğŸ”„ æ­£åœ¨æå–å•è¯...")
            try:
                words_data = extract_words_from_markdown(input_file, output_file, include_phrases=False)
                print(f"\nâœ… æˆåŠŸï¼")
                preview_results(words_data)
            except Exception as e:
                print(f"\nâŒ æå–å¤±è´¥: {e}")
        
        elif choice == '3':
            # å•è¯+çŸ­è¯­+è¯ä¹‰æ¨¡å¼
            default_output = f"{base_name}_å®Œæ•´.txt"
            output_file = get_output_file(default_output)
            
            print("\nğŸ”„ æ­£åœ¨æå–å•è¯å’ŒçŸ­è¯­...")
            try:
                words_data, phrases_data = extract_words_from_markdown(input_file, output_file, include_phrases=True)
                print(f"\nâœ… æˆåŠŸï¼")
                preview_results(words_data, phrases_data)
            except Exception as e:
                print(f"\nâŒ æå–å¤±è´¥: {e}")
        
        input("\nâœ¨ æŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == '__main__':
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # å‘½ä»¤è¡Œæ¨¡å¼
        parser = argparse.ArgumentParser(description='ä» PDF æˆ– Markdown æ–‡ä»¶ä¸­æå–å•è¯')
        parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ PDF å’Œ Markdownï¼‰')
        parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
        parser.add_argument('-m', '--mode', choices=['full', 'words_only'], default='words_only',
                            help='æå–æ¨¡å¼ï¼šfull=å®Œæ•´ä¿¡æ¯ï¼Œwords_only=ä»…å•è¯ï¼ˆé»˜è®¤ï¼‰')
        parser.add_argument('-p', '--phrases', action='store_true',
                            help='æ˜¯å¦åŒ…å«çŸ­è¯­ï¼ˆä»…åœ¨fullæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰')
        parser.add_argument('--no-unique', action='store_true',
                            help='ä¸å»é‡ï¼ˆä»…åœ¨words_onlyæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰')
        
        args = parser.parse_args()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.input_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            sys.exit(1)
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        is_pdf = args.input_file.lower().endswith('.pdf')
        
        if is_pdf:
            # å¤„ç† PDF æ–‡ä»¶
            print(f"ğŸ“„ æ£€æµ‹åˆ° PDF æ–‡ä»¶ï¼Œå°†é€šè¿‡ Mineru API å¤„ç†...")
            markdown_file = process_pdf_file(args.input_file, args.output)
            if markdown_file:
                print("\nâœ… PDF å¤„ç†å®Œæˆï¼å•è¯å·²è‡ªåŠ¨æå–å’Œæ ¸å¯¹")
            else:
                print("\nâŒ PDF å¤„ç†å¤±è´¥")
                sys.exit(1)
        else:
            # å¤„ç† Markdown æ–‡ä»¶
            # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
            if not args.output:
                if args.mode == 'full':
                    args.output = 'extracted_words_full.txt'
                else:
                    args.output = 'extracted_words.txt'
            
            # æ‰§è¡Œæå–
            if args.mode == 'full':
                extract_words_from_markdown(args.input_file, args.output, args.phrases)
            else:
                extract_words_only(args.input_file, args.output, not args.no_unique)
            
            print("\næå–å®Œæˆï¼")
    else:
        # äº¤äº’å¼æ¨¡å¼
        try:
            interactive_mode()
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")

